Task 1
#Importing Datasets
actual_data = pd.read_excel("hstarts_first_second_third.xlsx", 'DATA', skiprows=4)
forecasted_data = pd.read_excel("Individual_HOUSING.xlsx")

#Scale the data to be able to compare
forecast_scaling_factor = 1000 
forecasted_data["HOUSING2"] *= forecast_scaling_factor  # Used the initial dataset for further usage

#Change the Column names to be easier to merge 
actual_data.columns = ['Date', 'First_Update', 'Second_Update', 'Third_Update', 'Most_Recent'] 
# Rename specific columns in 'forecasted_data' to match merge keys
forecasted_data.rename(columns={'YEAR': 'Year', 'QUARTER': 'Quarter'}, inplace=True)
print(actual_data)

# Drop unnecessary columns from actual_data
actual_data.drop('First_Update', axis=1, inplace=True)
actual_data.drop('Second_Update', axis=1, inplace=True)
actual_data.drop('Third_Update', axis=1, inplace=True)
# Drop rows with missing values (NaNs) from both datasets
forecasted_data = forecasted_data.dropna()
actual_data = actual_data.dropna()
actual_data.head()

#Extract Years and Quarters from the 'Date' column

actual_data['Year'] = pd.to_datetime(actual_data['Date'], format='%Y:%m').dt.year # Extracting Years
actual_data['Quarter'] = pd.to_datetime(actual_data['Date'], format='%Y:%m').dt.quarter #Extracting Data
actual_data.head()

#Built a new dataframe for this question specifically
AD_Q2T1 = actual_data
print(AD_Q2T1)

# grouping by Year_Quarters and filtered the unnecesary columns
AD_Q2T1['Year_Quarter'] = actual_data['Year'].astype(str) + 'Q' + actual_data['Quarter'].astype(str) #Creating Year_Quarter column and addressing Year 
QD_Q2T1 = AD_Q2T1.groupby('Year_Quarter')[['Most_Recent']].mean() #Grouping by Quarter
# Drop the 'Date' column from actual_data as it's no longer needed
actual_data.drop('Date', axis=1, inplace=True)
# Set pandas to format float outputs to 2 decimal places

QD_Q2T1.head()

#Format date 
FD_Q2T1 = forecasted_data
FD_Q2T1['Year_Quarter'] = forecasted_data['Year'].astype(str) + 'Q' + forecasted_data['Quarter'].astype(str)
FD_Q2T1.tail()

# Drop unnecessary columns
columns_to_drop = ['Year','Quarter','HOUSING1','HOUSING3','HOUSING4','HOUSING5','HOUSING6','HOUSINGA','HOUSINGB']
FD_Q2T1.drop(columns=columns_to_drop) #To keep the integrity of the initial dataset for further use, a new one was created with the columns needed

#Removing unecessary columns and group by quarters
HOUSING2QuarterlyFD_1 = FD_Q2T1.groupby('Year_Quarter')[['HOUSING2']].mean() #Grouping by Quarter
# Display first 10 rows
HOUSING2QuarterlyFD_1.head()

#Merge both datasets
merged_data = QD_Q2T1.merge(HOUSING2QuarterlyFD_1, on='Year_Quarter', how='inner')
merged_data.head()

#Filter data of the Years for group 4 
QMD_group4 = merged_data.loc["2003Q1":"2015Q4"]
QMD_group4.head()

# Create interactive figure
fig = go.Figure()

# Most_Recent variable trace lines creation
fig.add_trace(go.Scatter(
    x=QMD_group4.index,
    y=QMD_group4['Most_Recent'],
    mode='lines+markers',
    name='Most_Recent',
    marker=dict(symbol='circle'),
    hovertemplate='Quarter: %{x}<br>Value: %<extra></extra>'
))

# HOUSING2 variable trace lines creation
fig.add_trace(go.Scatter(
    x=QMD_group4.index,
    y=QMD_group4['HOUSING2'],
    mode='lines+markers',
    name='HOUSING2',
    marker=dict(symbol='pentagon'),
    hovertemplate='Quarter: %{x}<br>Value: %<extra></extra>'
))

# Updating Layout of the graph
fig.update_layout(
    title='Comparison of Most_Recent and HOUSING2',
    xaxis_title='Year_Quarter',
    yaxis_title='Values',
    hovermode='x',
    legend=dict(x=0.01, y=0.99),
    template='plotly_white',
    height=500,
    width=900
)

# Show chart
fig.show()

Task 2 A)

#Filter data of the Years for group 4 
QMD_group4 = merged_data.loc["2003Q1":"2015Q4"]
QMD_group4.head()
Most_Recent	HOUSING2
Year_Quarter		
2003Q1	1736.00	1693.51
2003Q2	1753.67	1697.30
2003Q3	1889.67	1714.93
2003Q4	2035.67	1797.47
2004Q1	1918.33	1932.60
# Create interactive figure
fig = go.Figure()

# Most_Recent variable trace lines creation
fig.add_trace(go.Scatter(
    x=QMD_group4.index,
    y=QMD_group4['Most_Recent'],
    mode='lines+markers',
    name='Most_Recent',
    marker=dict(symbol='circle'),
    hovertemplate='Quarter: %{x}<br>Value: %<extra></extra>'
))

# HOUSING2 variable trace lines creation
fig.add_trace(go.Scatter(
    x=QMD_group4.index,
    y=QMD_group4['HOUSING2'],
    mode='lines+markers',
    name='HOUSING2',
    marker=dict(symbol='pentagon'),
    hovertemplate='Quarter: %{x}<br>Value: %<extra></extra>'
))

# Updating Layout of the graph
fig.update_layout(
    title='Comparison of Most_Recent and HOUSING2',
    xaxis_title='Year_Quarter',
    yaxis_title='Values',
    hovermode='x',
    legend=dict(x=0.01, y=0.99),
    template='plotly_white',
    height=500,
    width=900
)

# Show chart
fig.show()

Task 2
#Computing forecasters standard deviation among forecasters from different industries

#Building data frames for task 2
AD_Q2T2 = QD_Q2T1
FD_Q2T2 = FD_Q2T1
print("Actual Data:")
print(QD_Q2T1.head())

print("Forecasted Data:")
print(FD_Q2T2.head())

# Merge the actual (quarterly_actual) and forecasted data on Year, Quarter, and Year_Quarter
Q2T2_merged = AD_Q2T2.merge(FD_Q2T2, on=['Year_Quarter'], how='inner')

# Print the first few rows of the merged dataset
print("\nmerged_data head (after merging):")
print(Q2T2_merged.head())

# Filter merged_data to include only rows from 2003 to 2015 as previously was only filtered after merging
Q2T2_merged = Q2T2_merged[(Q2T2_merged['Year'] >= 2003) & (Q2T2_merged['Year'] <= 2015)]

# Calculate forecast error: percentage difference between forecasted and actual housing values
Q2T2_merged['Forecast_Error'] = (Q2T2_merged['HOUSING2'] / Q2T2_merged['Most_Recent']) - 1


print("\nmerged_data head with Forecast_Error:")

Q2T2_merged.head()

# Group merged data by INDUSTRY and calculate standard deviation of forecast errors for each
std_by_industry = Q2T2_merged.groupby('INDUSTRY')['Forecast_Error'].std()

print("\nStandard Deviation of Forecast Error by Industry:")
print(std_by_industry)

#Computing forecast errors across industries
quarterly_error = Q2T2_merged.groupby('Year_Quarter')['Forecast_Error'].mean()
# Create interactive line chart
fig = go.Figure()

# Add forecast error trace
fig.add_trace(go.Scatter(
    x=quarterly_error.index,
    y=quarterly_error.values,
    mode='lines+markers',
    name='Forecast Error',
    line=dict(color='blue'),
))

# Add horizontal reference line at y=0
fig.add_trace(go.Scatter(
    x=quarterly_error.index,
    y=[0]*len(quarterly_error),
    mode='lines',
    name='Perfect Forecast (Error = 0)',
    line=dict(color='red', dash='dash'),
    hoverinfo='skip'
))

# Layout customization
fig.update_layout(
    title='Forecast Accuracy Over Time (2003â€“2015)',
    xaxis_title='Year_Quarter',
    yaxis_title='Forecast Error (Forecast/Actual - 1)',
    hovermode='x',
    legend=dict(x=0.01, y=0.99),
    template='plotly_white',
    height=500,
    width=900
)

# Show the interactive chart
fig.show()

Section B)

#Regression Analysis on mean forecast changes
##ompute the mean forecasted
mean_forecast_by_quarter = merged_data.groupby('Year_Quarter')['HOUSING2'].mean().reset_index()
mean_actual_by_quarter = merged_data.groupby('Year_Quarter')['Most_Recent'].mean().reset_index()

# Display the mean forecast and actual data for verification
print("\nmean_forecast_by_quarter head:")
print(mean_forecast_by_quarter.head())
print("\nmean_actual_by_quarter head:")
print(mean_actual_by_quarter.head())

# Merge mean forecast and actual data
combined_data = mean_forecast_by_quarter.merge(mean_actual_by_quarter, on='Year_Quarter')

# Display combined_data after merging
print("\ncombined_data head:")
print(combined_data.head())

# Calculate quarterly changes
combined_data['Forecast_Change'] = combined_data['HOUSING2'].diff()
combined_data['Actual_Change'] = combined_data['Most_Recent'].diff()

# Display combined_data after calculating changes
print("\ncombined_data with changes head:")
print(combined_data.head())

# Drop NaN values after differencing
combined_data.dropna(inplace=True)

# Display combined_data after dropping NaNs
print("\ncombined_data after dropping NaNs head:")
print(combined_data.head())

# Regress mean forecast change on actual change
X = sm.add_constant(combined_data['Forecast_Change'])
y = combined_data['Actual_Change']

model = sm.OLS(y, X).fit()

print(model.summary())

Task 3)

#Regress forecasted housing data on SPY
## Load the  data
sp500_data = pd.read_csv("SPY.csv")
sp500_data= sp500_data.dropna()
# Convert 'Date' column to datetime format (automatically detects format)
sp500_data['Date'] = pd.to_datetime(sp500_data['Date'])

# Extract Year and Quarter
sp500_data['Year'] = sp500_data['Date'].dt.year
sp500_data['Quarter'] = sp500_data['Date'].dt.quarter

# Create Year_Quarter column
sp500_data['Year_Quarter'] = sp500_data['Year'].astype(str) + 'Q' + sp500_data['Quarter'].astype(str)

# Filter data between Q1 2003 and Q4 2015
sp500_data_filtered = sp500_data[(sp500_data['Year_Quarter'] >= '2003Q1') & (sp500_data['Year_Quarter'] <= '2015Q4')]

# Calculate percentage change in S&P 500 close price
sp500_data_filtered['Pct_Change_SP500'] = sp500_data_filtered['Close'].pct_change() * 100

# Group by Year_Quarter and calculate average percentage change
Quarterly_sp500 = sp500_data_filtered.groupby('Year_Quarter')['Pct_Change_SP500'].mean().reset_index()

# Display the first few rows
print(Quarterly_sp500.head())

QMD_group4['Pct_Change_housing_starts'] = QMD_group4['Most_Recent'].pct_change() * 100

#Merging datasets
merged_datareg = Quarterly_sp500.merge(QMD_group4, on='Year_Quarter', how='inner')
merged_datareg = merged_datareg.dropna()
merged_datareg.head()

X = merged_datareg['Pct_Change_housing_starts']
Y = merged_datareg['Pct_Change_SP500']

# Add constant for intercept
X = sm.add_constant(X)

# Run regression
model = sm.OLS(Y, X).fit()

# Print summary
print(model.summary())

Task 4
#Perform Individual level analysis on a forecaster basis

# Create a copy of the data
AD_Q2T4 = actual_data
FD_Q2T4 = forecasted_data
FD_Q2T4.head()


#Rename columns in FD_Q2T4
FD_Q2T4.rename(columns={'YEAR': 'Year', 'QUARTER': 'Quarter'}, inplace=True)

#
FD_Q2T4['Year_Quarter_ID'] = FD_Q2T4['Year'].astype(str) + 'Q' + FD_Q2T4['Quarter'].astype(str) + 'ID' + FD_Q2T4['ID'].astype(str)

AD_Q2T4 = AD_Q2T4.groupby(['Year', 'Quarter'])['Most_Recent'].mean().reset_index()


#Drop rows with NA in AD_Q2T4 and FD_Q2T4 (ensure to reassign it back)
AD_Q2T4 = AD_Q2T4.dropna()
FD_Q2T4 = FD_Q2T4.reset_index()
#Filter rows by 'Year' (2003 to 2015)
FD_Q2T4 = FD_Q2T4[(FD_Q2T4['Year'] >= 2003) & (FD_Q2T4['Year'] <= 2015)]
AD_Q2T4 = AD_Q2T4[(AD_Q2T4['Year'] >= 2003) & (AD_Q2T4['Year'] <= 2015)]

#Confirm column names and inspect the head of FD_Q2T4
FD_Q2T4.head()

#Merged data 
combined_Q2T4 = pd.merge(FD_Q2T4, AD_Q2T4[['Year', 'Quarter', 'Most_Recent']], 
                        how='inner')
column_droping = ['Year','Quarter','ID','INDUSTRY','HOUSING1','HOUSING3','HOUSING4','HOUSING5','HOUSING6','HOUSINGA','HOUSINGB',"Year_Quarter","index"]
combined_Q2T4.drop(columns = column_droping, inplace = True)
combined_Q2T4.head()

#Calculate Errors for further use
combined_Q2T4['Error'] = abs((combined_Q2T4['HOUSING2'] / combined_Q2T4['Most_Recent']) - 1)
print(combined_Q2T4)


# Calculated : MAE: Mean Absolute Error, RMSE: Root Mean Squared Error, MAPE: Mean Absolute Percentage Error
error_metrics = combined_Q2T4.groupby('Year_Quarter_ID').agg(
    MAE=('Error', lambda x: np.mean(np.abs(x)).round(6))
)
error_metrics['MAE'] = error_metrics['MAE'].round(6)

print(error_metrics)

#Finding the TOP5 MAE
top_10_MAE = error_metrics.sort_values(by = "MAE").head(10)
print(top_10_MAE)

#Building a plot for the top 10 Performers

ax = top_10_MAE.plot(kind='bar', stacked=True, figsize=(10, 6))

# Customize the chart
ax.set_title('Top 10 Performers: Bar Chart of MAE', fontsize=14)
ax.set_xlabel('Year_Quarter_ID', fontsize=12)
ax.set_ylabel('MAE', fontsize=12)
ax.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')
print(ax)

#Listing the TOP 5 Results, including Error 
top_10 = ["2005Q3ID535","2005Q3ID84","2015Q3ID433","2012Q3ID535","2012Q3ID420","2011Q1ID528","2010Q3ID546","2010Q3ID566","2005Q4ID533","2005Q4ID535"]
filtered_data = combined_Q2T4[combined_Q2T4['Year_Quarter_ID'].isin(top_10)]
print("The 10 Top forecasters:")
print(filtered_data)

#Finding the worst 5 forecasters
worst_10_MAE = error_metrics.sort_values(by = "MAE").tail(10)
print(worst_10_MAE)

#Building the chart for the TOP 10 Worst Performers 
bx = worst_10_MAE.plot(kind='bar', stacked=True, figsize=(10, 6))

# Customize the chart
bx.set_title('Worst 5 Performers:Bar Chart of MAE', fontsize=14)
bx.set_xlabel('Year_Quarter_ID', fontsize=12)
bx.set_ylabel('MAE', fontsize=12)
bx.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')
print(bx)

#Listing the Worst 5 Forecasters to be able to compare actual forecasts and data
worst_10 = ["2008Q4ID483","2008Q4ID99","2010Q4ID558","2009Q1ID504","2010Q3ID426","2009Q1ID541","2011Q1ID426","2008Q4ID531","2008Q4ID421","2010Q4ID426"]
filtered_worstdata = combined_Q2T4[combined_Q2T4['Year_Quarter_ID'].isin(worst_10)]
print("The worst 10 forecasters")
filtered_worstdata.head(10)

t_test_AD =combined_Q2T4["Most_Recent"].astype(float)  # Convert to float if necessary
t_test_FD =combined_Q2T4["HOUSING2"].astype(float)   # Convert to float if necessary

t_stat, p_value = stats.ttest_rel(t_test_AD, t_test_FD)
print(f"T-statistic: {t_stat}")
print(f"P-value: {p_value}")

# Interpret the p-value
alpha = 0.05  # 5% significance level
if p_value < alpha:
    print("There is a significant difference between forecasts and actual results.")
else:
    print("There is no significant difference between forecasts and actual results.")

#Analysis
#In the first column, the forecaster ID'd 535 appeared 3 times with one of the forecasts MAE being 0 as he had a 100% forecast being the best forecaster in the time 2003-2015 time frame. The second bar chart shows the worst 10 forecasts, the forecaster number 426 appears 3 times in the list of the worst 10 forecasters, being the forecaster with the biggest MAE between forecasters. The range of values from the worst 10, varying between 0.36 and 0.64 is much shorter than the top 10 as in the top which varied from 0 to 0.000645.
